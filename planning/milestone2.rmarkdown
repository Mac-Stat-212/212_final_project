---
title: "Group_Project_5year_Data"
format: html
editor: visual
---


## Quarto


```{r}
library(tidyverse)
library(ggplot2)
library(dplyr)
library(here)
```

```{r}
# Load the data
data <- read.csv("../data/All_Tickers_5_Year_Data.csv")
```

```{r}
data <- data[, colSums(is.na(data)) == 0]
```


The main feedback we got last time was to get the data. We were able to successfully load it and clean it.

Now, we must address that a significant portion of our data goes away after cleaning NA values. There are a multitude of reasons this can happen. I'll give 4 examples: 1) Some companies began trading on the stock market recently, so they do not have data trailing for 5 years. In other words, any company that started trading after October 2019 will not be on our dataset. 2) Some companies went to bankrupcy and got delisted during our study period. 3) Some companies were temporarily delisted while being investigating. 4) Data entry errors. We cannot know as of know which NA values belong to which category. With over 35 thousand stocks, this is a difficult taks. We will tackle it on the next milestone. 

Another feedback we got from Brianna was to further clarify the concept of anchoring bias.

An anchor is simply a reference point. For example, when a student goes to the library and tells herself "I'll study until 10pm". The time she sets as a limit is a reference point. Now, the anchoring effect "is a psychological phenomenon in which an individual's judgments or decisions are influenced by a reference point or 'anchor' which can be completely irrelevant" (wikipedia). May be studying until 10 pm makes sense because that's the time the library closes or because the bedtime of this particular student is 10:30 pm. Now, it could also be that people just like to organize their day around round numbers because that's the norm. Envision for example the student who by 9:50 pm is stressed because he still has one more problem to revise, but 10 pm is approaching. If the library is closing at let's say 11 pm, and she goes to bed at 12pm, there should be no reason why she can just stay a couple of extra minutes to finish her task. 

If that example was not clear enough, may be just think about why grocery stocks love to have their prices ending at 0.99. Is the average consumer more likely to complain if the price of their carton of egg goes from 3.83 to 3.99 or if it goes from 3.9 to 4.0? Chances are the latter will get more headlines and conversations, even though the former change was significantly more pronounced percentage wise. 

Research has shown that investors in the stock market also rely on certain reference points or anchors, one of which is the "52-week high"—the highest price a stock has reached in the past 52 weeks. Studies indicate that American investors are more likely than Chinese investors to sell their shares when a stock's price approaches this benchmark (Hao et al., 2018).

We, basically, want to investigate if this anchoring effect also applies when a stock price reaches for the first time in a while numbers that people find particularly relevant, like 100 or 13. 

For this milestone, we do a little exploration around the number 100.

Our sample consists of all stocks that traded between October 17 2017 and October 16 2018 and never reached a closing stock price of more than 50 dollars, and later on went to reach for the first time 100$ in the following years. There is a long way between 50 and 100, and in a perfect economically-efficient world, there should be not reason why reaching a particular number changes the price trend of different stocks.  

I sent the above write-up as a prompt to Gemini, and asked to produce code.


```{r}
library(dplyr)
library(purrr)
library(lubridate)

# Function to analyze stocks reaching $100 for the first time
analyze_stocks_reaching_100 <- function(df) {
  
  # Step 1: Filter for stocks never closing above $50 between Oct 17, 2019 and Oct 16, 2020
  start_date <- as.Date("2019-10-17")
  end_date <- as.Date("2020-10-16")
  
  # Identify all unique tickers by finding all columns that have "_Adj.Close" in their name
  tickers <- unique(gsub("_Adj.Close", "", colnames(df)[grepl("_Adj.Close", colnames(df))]))
  
  # Create a storage list to store the results
  analysis_results <- vector("list", length = length(tickers))
  names(analysis_results) <- tickers
  
  # Function to analyze a single ticker
  analyze_single_ticker <- function(ticker) {
    
    # Extract adjusted closing prices and dates
    adj_close_prices <- df[[paste0(ticker, "_Adj.Close")]]
    dates <- as.Date(df$Date)
    
    # Filter for the initial period (Oct 17, 2017 – Oct 16, 2018)
    initial_period <- which(dates >= start_date & dates <= end_date)
    
    # Skip this ticker if it closed above $50 during the initial period
    if (any(adj_close_prices[initial_period] > 50, na.rm = TRUE)) return(NULL)
    
    # Step 2: Find the first time the stock reaches $100 after Oct 16, 2018
    post_initial_period <- which(dates > end_date)
    first_time_reaching_100 <- which(adj_close_prices[post_initial_period] >= 100)[1]
    
    # If the stock never reaches $100, skip it
    if (is.na(first_time_reaching_100)) return(NULL)
    
    # Step 3: Analyze the day after the $100 breakthrough
    breakthrough_index <- post_initial_period[first_time_reaching_100]
    next_day_index <- breakthrough_index + 1
    
    # Make sure the next day exists
    if (next_day_index > length(adj_close_prices)) return(NULL)
    
    # Calculate the percentage change in adjusted closing price
    percentage_change <- ((adj_close_prices[next_day_index] - adj_close_prices[breakthrough_index]) / adj_close_prices[breakthrough_index]) * 100
    
    # Store the result as a list
    list(
      Ticker = ticker,
      Breakthrough_Date = dates[breakthrough_index],
      Breakthrough_Price = adj_close_prices[breakthrough_index],
      Next_Day_Price = adj_close_prices[next_day_index],
      Percentage_Change = percentage_change
    )
  }
  
  # Use map to apply the function to each ticker and filter out NULL results
  analysis_results <- map(tickers, analyze_single_ticker) %>% 
    compact()  # Remove NULL values
  
  # Step 4: Convert the results list to a data frame
  results_df <- bind_rows(analysis_results)
  
  return(results_df)
}

# Run the analysis function on the data dataframe
results <- analyze_stocks_reaching_100(data)

# Calculate median and mean of the next day's percentage change
mean_percentage_change <- mean(results$Percentage_Change, na.rm = TRUE)
median_percentage_change <- median(results$Percentage_Change, na.rm = TRUE)

# Display the results
cat("Mean next day percentage change:", mean_percentage_change, "%\n")
cat("Median next day percentage change:", median_percentage_change, "%\n")


```

The mean percentage change (negative) is incredibly high (-1.88% daily in the world of investing is huge if there is an actual casual relationship), although the median tells another story. We are looking forward to understand why is this the case and run some regression models to understand what's happening, as well as account for outliers. For a future visualization, it would be a good idea to see how investors behave around different "breakthrough prices" (let's say numbers like 13, 20, 30, 40, etc) and see if particular numbers are more relevant.  

Concrete steps for next milestone are:

1) Handle NA values in a way that minimizes bias. 

2) Create visualizations around at least 2 breakthrough prices (10 and 100).

3) Run regression models

4) As a stretch goal and following on Brianna's feedback, we will try to add some additional information about the stocks we are analyzing. For example, we would like to know if they are small, medium or large companies?. It'd also be really interesting to know whether they are based in USA or in China, and that way we could do an exploration around the number 13, since some cultures associate 13 with bad luck. 


References:

Hao, Y., Chou, R. K., Ko, K. C., & Yang, N. T. (2018). The 52-week high, momentum, and investor sentiment. International Review of Financial Analysis, 57, 167-183.


