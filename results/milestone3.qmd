---
title: "Milestone 3 Blog"
format: 
  html:
    embed_resources: true
---

```{r packages, include=FALSE}
library(tidyverse)
library(ggplot2)
```

## Introduction

For our COMP/STAT 212 semester project, we decided to investigate what is known as "anchoring bias" in the stock market. Broadly speaking, anchoring bias is when we base our judgments on certain numbers, "anchors", more because they are comfortable or convenient than that they are relevant to the decision at hand. For example, a study showed that when subjects were asked to estimate the percentage of African nations in the UN, their guesses were significantly different if they were first asked whether it was higher/lower than 10% versus 65% ([source](https://pmc.ncbi.nlm.nih.gov/articles/PMC6396698/)). In that case, the anchor was the first number they heard. What we are exploring is slightly different. At first, we wanted to focus on specific "nice" numbers, such as 10 or 100, or "unlucky" numbers such as 13, and see if trading activity changed markedly when stocks hit those potential anchors. However, our initial analysis deemed this was too sensitive to outliers. So instead, we pivoted to a slightly different, and more familiar, idea. You've probably noticed when you go into the store that most things aren't priced at an exact dollar value, but some number of dollars plus 99 cents. Personally, I've always thought this was a little ridiculous, but there is a measurable effect on people's perception of price at 4.99 vs 5. Surely, though, traders on the stock market are far too canny to fall for these marketing tricks, because there's so much more at stake. If a noticeable effect were observed, it could potentially be leveraged for great profit. Let's see what the data has to say.

```{r data}
dat_wider <- read_csv('Long_Data.csv')
```

## The Data

Getting a good dataset was initially quite challenging. Because stock-market data has financial implications, there are a lot of services offering convenient access for a fee, but free data on any reasonable scale is difficult to find. Eventually, however, we obtained a dataset containing data on all (!!) publicly traded stocks over the past five years. Unfortunately, we got what we paid for: the csv takes up almost an entire gigabyte, and presented some wrangling challenges, the first of which was the very large number of NA cells. For now, we just deleted any stock with any missing data whatsoever, but we may revisit this later to ensure that this doesn't bias the remaining data noticeably. What remained is still almost 45 million observations, covering daily lows, highs, opens, closes, adjusted closes and volumes for all remaining stocks, more than sufficient for this stage of the project.

```{r}
head(dat_wider, 10)
```


## Analysis

Measuring anchoring bias in the stock market is a little trickier than it sounds. We can't literally analyze how many shares were bought at specific prices, because that dataset would be mind-bogglingly huge, if we could somehow manage to find it. Instead, we had to use the best metric we have access to, which is daily lows. It's not perfect, because sometimes if a stock has a really rough day, the low might be the close, or vice versa, but generally speaking, the low is when people start to think that it is worth buying, and it begins to swing in the other direction. So, we simply counted how many lows were of each cent value, and plotted them.

```{r, include=FALSE}
dat_rounded_low <- mutate(dat_wider, rounded=cut(Low - floor(Low), seq(0, 1, by=0.1), right=FALSE))


low_ranges <- dat_rounded_low %>% count(rounded) %>% filter(!is.na(rounded))

low_ranges_percent <- low_ranges %>%
  mutate(percent_ranges = n/sum(n))
```

```{r}
low_ranges %>% 
  ggplot() +
  geom_col(aes(x = rounded, y = n)) +
  labs(x = "Price Range", y = "Recorded Lows") +
  theme_minimal()
```

```{r}
dat_wider %>% 
  mutate(Low = round(Low, 2)) %>% 
  mutate(Low = as.character(Low)) %>% 
  mutate(low_cat = ifelse(str_detect(Low, "\\..$"), paste0(str_sub(Low, -1, -1), "0"), str_sub(Low, -2, -1))) %>%
  mutate(low_cat = ifelse(str_detect(Low, "\\."), low_cat, 0)) %>% 
  group_by(low_cat) %>% 
  count() %>% 
  filter(!is.na(low_cat)) %>% 
  mutate(low_cat = as.numeric(low_cat)) %>% 
  # filter(low_cat < 10) %>% 
  # mutate(low_cat = as.character(low_cat)) %>% 
  ggplot() +
  geom_line(aes(x = low_cat, y = n)) +
  labs(x = "Cent Price", y = "Recorded Lows") +
  theme_minimal()
```

## Takeaways

These graphs are quite striking. The lows are far more clustered than they "should" be, considering that the differences are literally pocket change. The second graph is perhaps even too striking; the very high value at exactly 0 seems to indicate that some stocks might be recorded in this dataset without a cent value, which we will check out in the near future. But it doesn't seem like there's any reason why so many stocks have local minima at 50 cent values, or 20, or 60, except for the comfort of round numbers. And the fact that in both graphs the patterns are so regular implies that there are very real relationships in play here. For the rest of the semester, we will focus on refining and teasing out these relationships. Is there a similar trend considering dollar values instead of cents? Does this change depending on the size and location of the companies involved? Do we observe similar trends using other metrics, such as daily highs or closes? And is bias of any kind affecting our results? These are the questions we hope to answer moving forward.

```{r}
sessionInfo()
```
